{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEsLHl7Qy4f8",
        "outputId": "93669f51-c5fd-404f-aa51-fbc1e1852b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.233-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Downloading ultralytics-8.3.233-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.3.233 ultralytics-thop-2.0.18\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics torch torchvision torchaudio scikit-learn opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "import os, glob, math, json, time, collections, random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================================\n",
        "# UTILITIES\n",
        "# ==========================================================\n",
        "\n",
        "def make_dirs(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def timestamp():\n",
        "    return time.strftime('%Y%m%d_%H%M%S')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nB2EP98zfho",
        "outputId": "bbcc563a-0d8b-41a7-f47a-993e08639e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Cropper\n"
      ],
      "metadata": {
        "id": "QKAD3Qsr0nSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# VIDEO CROPPER\n",
        "# ============================================\n",
        "\n",
        "class VideoCropper:\n",
        "    def __init__(self, model_name='yolo11n.pt', device=None):\n",
        "        print(f\"Loading YOLO model: {model_name}...\")\n",
        "        try:\n",
        "            self.model = YOLO(model_name)\n",
        "            print(f\"✓ Loaded {model_name}\")\n",
        "        except Exception:\n",
        "            # Fallback to nano model\n",
        "            self.model = YOLO('yolo11n.pt')\n",
        "            print(\"✓ Loaded yolo11n.pt (fallback)\")\n",
        "\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def crop_player(self, frame):\n",
        "        \"\"\"Detect person and return cropped frame. Falls back to center crop.\"\"\"\n",
        "        try:\n",
        "            results = self.model(frame, imgsz=640, verbose=False)\n",
        "\n",
        "            # Check if valid results\n",
        "            if not results or not results[0].boxes or len(results[0].boxes) == 0:\n",
        "                return self._center_crop(frame)\n",
        "\n",
        "            # Get first detection (assume it's a person)\n",
        "            box = results[0].boxes[0]\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
        "\n",
        "            # Validate box size\n",
        "            h, w = frame.shape[:2]\n",
        "            x1, y1 = max(0, x1), max(0, y1)\n",
        "            x2, y2 = min(w, x2), min(h, y2)\n",
        "\n",
        "            if x2 - x1 < 20 or y2 - y1 < 20:\n",
        "                return self._center_crop(frame)\n",
        "\n",
        "            return frame[y1:y2, x1:x2]\n",
        "\n",
        "        except Exception:\n",
        "            return self._center_crop(frame)\n",
        "\n",
        "    def _center_crop(self, frame):\n",
        "        \"\"\"Fallback: crop center square of frame.\"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "        size = min(h, w)\n",
        "        y1 = (h - size) // 2\n",
        "        x1 = (w - size) // 2\n",
        "        return frame[y1:y1+size, x1:x1+size]\n"
      ],
      "metadata": {
        "id": "j-y2LWupzjQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdaptiveKeyframeExtractor:\n",
        "    \"\"\"\n",
        "    Adaptive keyframe extraction for 1-3 second action videos.\n",
        "\n",
        "    Strategy:\n",
        "    - < 1s: Use ALL frames + repeat to reach target\n",
        "    - 1-2s: Uniform sampling\n",
        "    - > 2s: Dense sampling in middle 60% (action region)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, target_frames=16):\n",
        "        self.target_frames = target_frames\n",
        "\n",
        "    def extract_frame_indices(self, video_path):\n",
        "        \"\"\"\n",
        "        Get frame indices to extract based on video duration.\n",
        "\n",
        "        Returns:\n",
        "            List of frame indices\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        duration = total_frames / fps if fps > 0 else 0\n",
        "        cap.release()\n",
        "\n",
        "        if duration < 1.0:\n",
        "            # Very short: use ALL frames\n",
        "            indices = list(range(total_frames))\n",
        "\n",
        "            # Repeat frames to reach target\n",
        "            if len(indices) < self.target_frames:\n",
        "                repeat_factor = self.target_frames / len(indices)\n",
        "                new_indices = []\n",
        "                for i in indices:\n",
        "                    n_repeats = int(np.ceil(repeat_factor))\n",
        "                    new_indices.extend([i] * n_repeats)\n",
        "                indices = new_indices[:self.target_frames]\n",
        "\n",
        "        elif duration <= 2.0:\n",
        "            # Medium: uniform sampling\n",
        "            if total_frames <= self.target_frames:\n",
        "                indices = list(range(total_frames))\n",
        "            else:\n",
        "                step = total_frames / self.target_frames\n",
        "                indices = [int(i * step) for i in range(self.target_frames)]\n",
        "\n",
        "        else:\n",
        "            # Longer: dense middle sampling (skip first 20% and last 20%)\n",
        "            start_frame = int(total_frames * 0.2)\n",
        "            end_frame = int(total_frames * 0.8)\n",
        "            action_frames = end_frame - start_frame\n",
        "\n",
        "            step = max(1, action_frames // self.target_frames)\n",
        "            indices = [start_frame + i * step for i in range(self.target_frames)]\n",
        "\n",
        "        return indices[:self.target_frames]"
      ],
      "metadata": {
        "id": "859qzA1e_KT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# KEYPOINT EXTRACTOR\n",
        "# ============================================\n",
        "\n",
        "class KeypointExtractor:\n",
        "    \"\"\"Extract human pose keypoints using YOLO11 pose model.\"\"\"\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        from ultralytics import YOLO\n",
        "        print(\"Loading YOLO11 Pose model...\")\n",
        "        self.model = YOLO('yolo11n-pose.pt')  # Nano pose model\n",
        "        self.device = device\n",
        "        self.feature_dim = 51  # 17 keypoints × 3 (x, y, confidence)\n",
        "\n",
        "    def extract(self, frame):\n",
        "        \"\"\"Extract keypoints from frame.\n",
        "\n",
        "        Returns:\n",
        "            np.array of shape (51,): flattened [x1,y1,conf1, x2,y2,conf2, ...]\n",
        "        \"\"\"\n",
        "        try:\n",
        "            results = self.model(frame, imgsz=640, verbose=False)\n",
        "\n",
        "            if len(results) == 0 or not hasattr(results[0], 'keypoints'):\n",
        "                return np.zeros(self.feature_dim, dtype=np.float32)\n",
        "\n",
        "            kpts = results[0].keypoints\n",
        "\n",
        "            if kpts is None or len(kpts) == 0:\n",
        "                return np.zeros(self.feature_dim, dtype=np.float32)\n",
        "\n",
        "            # Get first person's keypoints\n",
        "            kpt_data = kpts.data[0].cpu().numpy()  # Shape: (17, 3)\n",
        "\n",
        "            # Normalize coordinates by frame dimensions\n",
        "            h, w = frame.shape[:2]\n",
        "            kpt_data[:, 0] /= w  # Normalize x\n",
        "            kpt_data[:, 1] /= h  # Normalize y\n",
        "\n",
        "            # Flatten to 1D\n",
        "            features = kpt_data.flatten()\n",
        "\n",
        "            return features.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Keypoint extraction failed: {e}\")\n",
        "            return np.zeros(self.feature_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "# ==========================================================\n",
        "# FEATURE CACHE\n",
        "# ==========================================================\n",
        "\n",
        "class VideoFeatureCache:\n",
        "    \"\"\"Cache extracted features to disk to speed up training.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def cache_path(self, video_path):\n",
        "        fname = Path(video_path).stem + '.npy'\n",
        "        return self.cache_dir / fname\n",
        "\n",
        "    def exists(self, video_path):\n",
        "        return self.cache_path(video_path).exists()\n",
        "\n",
        "    def save(self, video_path, arr):\n",
        "        try:\n",
        "            np.save(str(self.cache_path(video_path)), arr)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to save cache for {video_path}: {e}\")\n",
        "\n",
        "    def load(self, video_path):\n",
        "        try:\n",
        "            return np.load(str(self.cache_path(video_path)))\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load cache for {video_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# ADAPTIVE FEATURE EXTRACTION\n",
        "# ============================================\n",
        "\n",
        "def precompute_keypoint_features_adaptive(videos, extractor, cache, seq_length=16):\n",
        "    \"\"\"Pre-extract keypoint features with adaptive sampling.\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ADAPTIVE KEYFRAME EXTRACTION + FEATURE EXTRACTION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    keyframe_extractor = AdaptiveKeyframeExtractor(target_frames=seq_length)\n",
        "    feature_dim = extractor.feature_dim\n",
        "    videos_to_process = [v for v in videos if not cache.exists(v)]\n",
        "\n",
        "    if len(videos_to_process) == 0:\n",
        "        print(\"✓ All features already cached!\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {len(videos_to_process)} videos...\\n\")\n",
        "\n",
        "    for video_path in tqdm(videos_to_process, desc=\"Extracting features\"):\n",
        "        try:\n",
        "            # Get adaptive frame indices\n",
        "            frame_indices = keyframe_extractor.extract_frame_indices(video_path)\n",
        "\n",
        "            # Extract features from selected frames\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            feats = []\n",
        "\n",
        "            for frame_idx in frame_indices:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ok, frame = cap.read()\n",
        "\n",
        "                if not ok:\n",
        "                    break\n",
        "\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                feat = extractor.extract(frame)\n",
        "                feats.append(feat)\n",
        "\n",
        "            cap.release()\n",
        "\n",
        "            # Pad if needed (shouldn't happen with adaptive)\n",
        "            if len(feats) < seq_length:\n",
        "                if len(feats) == 0:\n",
        "                    feats = [np.zeros(feature_dim, dtype=np.float32)] * seq_length\n",
        "                else:\n",
        "                    last = feats[-1]\n",
        "                    while len(feats) < seq_length:\n",
        "                        feats.append(last.copy())\n",
        "\n",
        "            arr = np.stack(feats[:seq_length], axis=0).astype(np.float32)\n",
        "            cache.save(video_path, arr)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError: {video_path}: {e}\")\n",
        "            zero_features = np.zeros((seq_length, feature_dim), dtype=np.float32)\n",
        "            cache.save(video_path, zero_features)\n",
        "\n",
        "    print(\"\\n✓ Feature extraction complete!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UHpB5GIP0K7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset + Augmentation + load pre-extracted features"
      ],
      "metadata": {
        "id": "KKjtWgOX0kI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PickleballDataset(Dataset):\n",
        "\n",
        "    def __init__(self, videos, labels, cache, seq_length=16,\n",
        "                 augment=False, feature_dim=2048):\n",
        "        self.videos = videos\n",
        "        self.labels = labels\n",
        "        self.cache = cache\n",
        "        self.seq_length = seq_length\n",
        "        self.augment = augment\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    def temporal_augmentation(self, features):\n",
        "        \"\"\"Apply temporal augmentation to feature sequence.\"\"\"\n",
        "        seq_len = features.shape[0]\n",
        "\n",
        "        # Random temporal shift (±20% of sequence)\n",
        "        if random.random() < 0.5:\n",
        "            shift = random.randint(-seq_len // 5, seq_len // 5)\n",
        "            if shift > 0:\n",
        "                # Shift right: pad left, trim right\n",
        "                features = np.concatenate([\n",
        "                    np.repeat(features[0:1], shift, axis=0),\n",
        "                    features[:-shift]\n",
        "                ], axis=0)\n",
        "            elif shift < 0:\n",
        "                # Shift left: trim left, pad right\n",
        "                features = np.concatenate([\n",
        "                    features[-shift:],\n",
        "                    np.repeat(features[-1:], -shift, axis=0)\n",
        "                ], axis=0)\n",
        "\n",
        "        # Random temporal dropout (drop 1-2 frames, duplicate neighbors)\n",
        "        if random.random() < 0.3:\n",
        "            num_drops = random.randint(1, 2)\n",
        "            for _ in range(num_drops):\n",
        "                drop_idx = random.randint(1, seq_len - 2)\n",
        "                # Replace dropped frame with average of neighbors\n",
        "                features[drop_idx] = (features[drop_idx-1] + features[drop_idx+1]) / 2\n",
        "\n",
        "        # Random temporal reverse (for symmetric actions)\n",
        "        if random.random() < 0.2:\n",
        "            features = features[::-1].copy()\n",
        "\n",
        "        return features\n",
        "\n",
        "    def add_feature_noise(self, x, std=0.01):\n",
        "        \"\"\"Add small Gaussian noise to precomputed features.\"\"\"\n",
        "        noise = np.random.normal(0, std, x.shape).astype(np.float32)\n",
        "        return x + noise\n",
        "\n",
        "    def frame_dropout(self, x, max_drops=3):\n",
        "        L = x.shape[0]\n",
        "        drops = random.randint(1, max_drops)\n",
        "        idxs = np.random.choice(L, drops, replace=False)\n",
        "        x[idxs] = 0\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        v = self.videos[idx]\n",
        "\n",
        "        # Load from cache\n",
        "        x = self.cache.load(v)\n",
        "\n",
        "        if x is None:\n",
        "            # Fallback: return zeros if cache missing\n",
        "            print(f\"Warning: Cache missing for {v}, using zeros\")\n",
        "            x = np.zeros((self.seq_length, self.feature_dim), dtype=np.float32)\n",
        "\n",
        "        # Apply augmentation during training\n",
        "        if self.augment:\n",
        "            x = self.temporal_augmentation(x)\n",
        "\n",
        "        if self.augment and random.random() < 0.5:\n",
        "            x = self.add_feature_noise(x, std=0.02)\n",
        "\n",
        "        if self.augment and random.random() < 0.3:\n",
        "            x = self.frame_dropout(x)\n",
        "\n",
        "\n",
        "\n",
        "        y = int(self.labels[idx])\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "Axgzbt7Y0gVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "GX87lsVW2ert"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, math, json, time, collections, random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "\n",
        "import torch, torch.nn as nn, torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from ultralytics import YOLO\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "\n",
        "\n",
        "\n",
        "# Utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ==========================================================\n",
        "# UTILITIES\n",
        "# ==========================================================\n",
        "\n",
        "def make_dirs(d):\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "def timestamp():\n",
        "    return time.strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# ============================================\n",
        "# UPDATED MODEL (smaller LSTM for keypoints)\n",
        "# ============================================\n",
        "\n",
        "class KeypointLSTMClassifier(nn.Module):\n",
        "    \"\"\"Lightweight LSTM for keypoint sequences.\"\"\"\n",
        "\n",
        "    def __init__(self, feature_dim=51, hidden=128, num_layers=1,\n",
        "                 num_classes=3, dropout=0.2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            feature_dim,\n",
        "            hidden,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln = nn.LayerNorm(hidden)\n",
        "        self.classifier = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (h, c) = self.lstm(x)\n",
        "        last_hidden = h[-1]\n",
        "        last_hidden = self.dropout(last_hidden)\n",
        "        last_hidden = self.ln(last_hidden)\n",
        "        logits = self.classifier(last_hidden)\n",
        "        return logits\n",
        "#Earlystop\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stopping to prevent overfitting.\"\"\"\n",
        "\n",
        "    def __init__(self, patience=7, min_delta=0.0, mode='min'):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.mode = mode\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, score):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            return False\n",
        "\n",
        "        if self.mode == 'min':\n",
        "            improved = score < (self.best_score - self.min_delta)\n",
        "        else:\n",
        "            improved = score > (self.best_score + self.min_delta)\n",
        "\n",
        "        if improved:\n",
        "            self.best_score = score\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "        return self.early_stop\n",
        "\n",
        "def list_videos(root, classes):\n",
        "    \"\"\"List all video files and their labels.\"\"\"\n",
        "    vids, labels, counts = [], [], {}\n",
        "\n",
        "    for ci, cname in enumerate(classes):\n",
        "        p = Path(root) / cname\n",
        "        if not p.exists():\n",
        "            print(f\"Warning: {p} does not exist\")\n",
        "            counts[cname] = 0\n",
        "            continue\n",
        "\n",
        "        files = list(p.glob('*.mp4')) + list(p.glob('*.avi')) + list(p.glob('*.MOV'))\n",
        "        vids.extend([str(x) for x in files])\n",
        "        labels.extend([ci] * len(files))\n",
        "        counts[cname] = len(files)\n",
        "\n",
        "    return vids, labels, counts\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# UPDATED TRAINING FUNCTION\n",
        "# ============================================\n",
        "\n",
        "def train_adaptive_model(params):\n",
        "    \"\"\"Train model with adaptive keyframe extraction.\"\"\"\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Initialize\n",
        "    print(\"\\nInitializing YOLO Pose model...\")\n",
        "    extractor = KeypointExtractor(device=device)\n",
        "    cache = VideoFeatureCache(params['cache_dir'])\n",
        "\n",
        "    # Load data\n",
        "    print(\"\\nLoading video data...\")\n",
        "    vids, labels, counts = list_videos(params['data_root'], params['classes'])\n",
        "    print(f\"Dataset: {counts}\")\n",
        "    print(f\"Total videos: {len(vids)}\")\n",
        "\n",
        "    if len(vids) == 0:\n",
        "        raise ValueError(\"No videos found! Check data_root path.\")\n",
        "\n",
        "    tr_v, val_v, tr_y, val_y = train_test_split(\n",
        "        vids, labels, test_size=0.2, stratify=labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain: {len(tr_v)} videos\")\n",
        "    print(f\"Val: {len(val_v)} videos\")\n",
        "\n",
        "    # Pre-extract with adaptive sampling\n",
        "    all_videos = list(set(tr_v + val_v))\n",
        "    precompute_keypoint_features_adaptive(\n",
        "        all_videos, extractor, cache, seq_length=params['seq_length']\n",
        "    )\n",
        "\n",
        "    # Free memory\n",
        "    del extractor\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Create datasets\n",
        "    train_ds = PickleballDataset(\n",
        "        tr_v, tr_y, cache,\n",
        "        seq_length=params['seq_length'],\n",
        "        augment=True,\n",
        "        feature_dim=51\n",
        "    )\n",
        "    val_ds = PickleballDataset(\n",
        "        val_v, val_y, cache,\n",
        "        seq_length=params['seq_length'],\n",
        "        augment=False,\n",
        "        feature_dim=51\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=params['batch_size'],\n",
        "                             shuffle=True, num_workers=0)\n",
        "    val_loader = DataLoader(val_ds, batch_size=params['batch_size'],\n",
        "                           shuffle=False, num_workers=0)\n",
        "\n",
        "    # Initialize model\n",
        "    print(\"\\nInitializing Keypoint LSTM model...\")\n",
        "    model = KeypointLSTMClassifier(\n",
        "        feature_dim=51,\n",
        "        hidden=128,\n",
        "        num_layers=1,\n",
        "        num_classes=len(params['classes']),\n",
        "        dropout=0.3\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=params['lr'],\n",
        "        weight_decay=0.01\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=10, min_delta=0.001, mode='min')\n",
        "\n",
        "    # Training\n",
        "    best_val_loss = float('inf')\n",
        "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STARTING TRAINING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for epoch in range(params['epochs']):\n",
        "        print(f\"\\nEpoch {epoch+1}/{params['epochs']}\")\n",
        "\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        for X, y in tqdm(train_loader, desc='Training', leave=False):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        avg_train_loss = np.mean(train_losses)\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in tqdm(val_loader, desc='Validation', leave=False):\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                outputs = model(X)\n",
        "                loss = criterion(outputs, y)\n",
        "                val_losses.append(loss.item())\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = np.mean(val_losses)\n",
        "        val_acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"Val Loss:   {avg_val_loss:.4f}\")\n",
        "        print(f\"Val Acc:    {val_acc*100:.2f}%\")\n",
        "\n",
        "        # Classification report every 5 epochs\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(\n",
        "                all_labels, all_preds,\n",
        "                target_names=params['classes'],\n",
        "                zero_division=0\n",
        "            ))\n",
        "\n",
        "        scheduler.step(avg_val_loss)\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Save best\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_val_acc = val_acc\n",
        "            torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'params': params,\n",
        "                'val_loss': avg_val_loss,\n",
        "                'val_acc': val_acc,\n",
        "            }, params['out_dir'] + '/best_adaptive_model.pth')\n",
        "            print(\"✓ Saved best model\")\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping(avg_val_loss):\n",
        "            print(f\"\\n⚠ Early stopping triggered at epoch {epoch+1}\")\n",
        "            break\n",
        "\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"TRAINING COMPLETE!\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best Val Loss: {best_val_loss:.4f}\")\n",
        "    print(f\"Best Val Acc:  {best_val_acc*100:.2f}%\")\n",
        "    print(f\"Total Epochs:  {len(history['train_loss'])}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# HOW TO USE\n",
        "# ============================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    params = {\n",
        "        'data_root': '/content/drive/MyDrive/DatasetVideoPickle',\n",
        "        'classes': ['Serve', 'DriveBackhand', 'DriveForehand'],\n",
        "        'seq_length': 16,\n",
        "        'batch_size': 16,  # Can use larger batch now\n",
        "        'lr': 0.001,\n",
        "        'epochs': 50,\n",
        "        'out_dir': '/content/drive/MyDrive/keypoint_experiments',\n",
        "        'cache_dir': '/content/drive/MyDrive/keypoint_cache',\n",
        "    }\n",
        "\n",
        "    os.makedirs(params['out_dir'], exist_ok=True)\n",
        "    os.makedirs(params['cache_dir'], exist_ok=True)\n",
        "\n",
        "    model, history = train_adaptive_model(params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "AtH36Yhv2dp9",
        "outputId": "7f244ae2-babe-4e3e-cd74-31aac9110829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Initializing YOLO Pose model...\n",
            "Loading YOLO11 Pose model...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt to 'yolo11n-pose.pt': 100% ━━━━━━━━━━━━ 6.0MB 100.3MB/s 0.1s\n",
            "\n",
            "Loading video data...\n",
            "Dataset: {'Serve': 283, 'DriveBackhand': 241, 'DriveForehand': 311}\n",
            "\n",
            "============================================================\n",
            "PRE-EXTRACTING KEYPOINT FEATURES\n",
            "============================================================\n",
            "Processing 835 videos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting keypoints:   2%|▏         | 20/835 [01:14<50:33,  3.72s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1449443922.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cache_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_keypoint_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1449443922.py\u001b[0m in \u001b[0;36mtrain_keypoint_model\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Pre-extract keypoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mall_videos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     precompute_keypoint_features(\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mall_videos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     )\n",
            "\u001b[0;32m/tmp/ipython-input-81309632.py\u001b[0m in \u001b[0;36mprecompute_keypoint_features\u001b[0;34m(videos, extractor, cache, seq_length)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos_to_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Extracting keypoints\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_FRAME_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}